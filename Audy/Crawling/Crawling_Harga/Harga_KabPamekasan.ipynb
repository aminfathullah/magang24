{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CRAWLING DATA HARGA KONSUMEN KABUPATEN PAMEKASAN\n",
    "> Penarikan data Harga Konsumen (Area) pada 1 tahun terakhir menggunakan metode MultiThread\n",
    "\n",
    "Data di ambil dari  ```https://siskaperbapo.jatimprov.go.id/harga/tabel.nodesign/``` <br>\n",
    "\n",
    "Penarikan data menggunakan Method POST<br>\n",
    "dengan menggunakan Payload/data \n",
    "```\n",
    "{\n",
    "    tanggal: 2024-04-30\n",
    "    kabkota: pamekasankab\n",
    "    pasar: \n",
    "}\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from queue import Queue, Empty\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "class MultiThreadedCrawler:\n",
    "    def __init__(self, seed_url):\n",
    "        self.seed_url = seed_url\n",
    "        self.root_url = '{}://{}'.format(urlparse(self.seed_url).scheme, urlparse(self.seed_url).netloc)\n",
    "        self.pool = ThreadPoolExecutor(max_workers=5)\n",
    "        self.scraped_pages = set([])\n",
    "        self.crawl_queue = Queue()\n",
    "        self.data = []  # Initialize an empty list to store scraped data\n",
    "\n",
    "    def parse_links(self, html):\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        Anchor_Tags = soup.find_all('a', href=True)\n",
    "        for link in Anchor_Tags:\n",
    "            url = link['href']\n",
    "            try:\n",
    "                res = requests.get(url, timeout=(3, 30))\n",
    "                return res\n",
    "            except requests.RequestException:\n",
    "                return\n",
    "\n",
    "    def scrape_page(self, url, tanggal):\n",
    "        try:\n",
    "            res = requests.post(urljoin(self.seed_url, url), data={'tanggal': tanggal, 'kabkota': 'pamekasankab', 'pasar': ''})\n",
    "            return res, tanggal\n",
    "        except requests.RequestException as e:\n",
    "            print(\"Request failed:\", e)\n",
    "            return None, tanggal\n",
    "\n",
    "    def post_scrape_callback(self, res, tanggal):\n",
    "        try:\n",
    "            if res:\n",
    "                soup = BeautifulSoup(res.text, 'html.parser')\n",
    "                table = soup.find('table', {'class': 'table table-bordered table-hover table-condensed'})\n",
    "                if table:\n",
    "                    header = [th.text.strip() for th in table.find_all('th')]\n",
    "                    data = []\n",
    "                    for row in table.find_all('tr')[1:]:\n",
    "                        cells = [td.text.strip() for td in row.find_all('td')]\n",
    "                        if cells:\n",
    "                            cells.append(tanggal)  # Tambahkan tanggal ke setiap baris data\n",
    "                            data.append(cells)\n",
    "                    df = pd.DataFrame(data, columns=header + ['Tanggal'])\n",
    "                    df['NO'] = df['NO'].ffill()   # Fill empty 'NO' cells with previous non-empty value\n",
    "                    self.data.append(df)  # Append the DataFrame to the list\n",
    "        except Exception as e:\n",
    "            print(\"Error during callback:\", e)\n",
    "\n",
    "    def run_web_crawler(self):\n",
    "        today = datetime.now()\n",
    "        for i in range(365):\n",
    "            tanggal = (today - timedelta(days=i)).strftime('%Y-%m-%d')  # Perulangan mundur\n",
    "            target_url = '/harga/tabel.nodesign/'\n",
    "            try:\n",
    "                res, tanggal = self.scrape_page(target_url, tanggal)\n",
    "                self.post_scrape_callback(res, tanggal)\n",
    "            except requests.RequestException as e:\n",
    "                print(\"Request failed:\", e)\n",
    "\n",
    "    def display_data(self):\n",
    "        if self.data:\n",
    "            today = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            print(f\"Harga Rata-Rata Kabupaten Pamekasan di Tingkat Konsumen\")\n",
    "            print(\"Pasar : Pasar Kolpajung, Pasar Waru\")\n",
    "            for df in self.data:\n",
    "                print(f\"Tanggal: {df['Tanggal'].iloc[0]}\")\n",
    "                print(df.drop(columns=['Tanggal']).to_string(index=False, justify='center', col_space=2, line_width=120))\n",
    "                print(\"\\n---\\n\")  # Pisahkan setiap hasil dengan tulisan ---\n",
    "            # Simpan data ke file Parquet\n",
    "            print(f\"Total Data: {total_rows} rows x {self.data[0].shape[1]} columns\\n\")\n",
    "            self.save_to_parquet()\n",
    "\n",
    "    def save_to_parquet(self):\n",
    "        if self.data:\n",
    "            # Gabungkan semua data dalam satu DataFrame\n",
    "            concatenated_df = pd.concat(self.data, ignore_index=True)\n",
    "            # Simpan DataFrame ke file Parquet\n",
    "            file_path = '../Output_Crawling/output_harga_kabPamekasan.parquet'\n",
    "            pq.write_table(pa.Table.from_pandas(concatenated_df), file_path)\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "# Menggunakan kelas MultiThreadedCrawler dengan URL seed yang diinginkan\n",
    "if __name__ == \"__main__\":\n",
    "    crawler = MultiThreadedCrawler('https://siskaperbapo.jatimprov.go.id/harga/')\n",
    "    crawler.run_web_crawler()\n",
    "    crawler.display_data()  # Call the display_data method to print the scraped data\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crawling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
